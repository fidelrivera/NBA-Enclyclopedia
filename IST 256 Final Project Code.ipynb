{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nba_api\n",
      "  Downloading nba_api-1.1.9-py3-none-any.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from nba_api) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->nba_api) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->nba_api) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->nba_api) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->nba_api) (4.0.0)\n",
      "Installing collected packages: nba-api\n",
      "Successfully installed nba-api-1.1.9\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.8/site-packages (from wikipedia) (4.9.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wikipedia) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->wikipedia) (2.0.1)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11686 sha256=9c51e15155582255f969122f48071c7c39393e5baa8a27d2a0193260b9b12f1c\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/07/93/05/72c05349177dca2e0ba31a33ba4f7907606f7ddef303517c6a\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.8/site-packages (4.2.1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>FG</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>...</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ray Allen*</td>\n",
       "      <td>SG</td>\n",
       "      <td>27</td>\n",
       "      <td>TOT</td>\n",
       "      <td>76</td>\n",
       "      <td>75</td>\n",
       "      <td>37.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>17.9</td>\n",
       "      <td>.439</td>\n",
       "      <td>...</td>\n",
       "      <td>.916</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Player Pos Age   Tm   G  GS    MP   FG   FGA   FG%  ...   FT%  ORB  \\\n",
       "4  Ray Allen*  SG  27  TOT  76  75  37.9  7.9  17.9  .439  ...  .916  1.2   \n",
       "\n",
       "   DRB  TRB  AST  STL  BLK  TOV   PF   PTS  \n",
       "4  3.8  5.0  4.4  1.4  0.2  2.6  2.9  22.5  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2020-21 NBA PLAYER INDEX/COMPARISON TOOL, A PROGRAM BY FIDEL RIVERA\n",
    "\n",
    "#INSTALLS\n",
    "\n",
    "!pip install nba_api #API for lots of useful nba data.\n",
    "!pip install wikipedia #API for lots of useful data about the individual. We use this to pull up a player description of the NBA Player inputted by the User\n",
    "!pip install lxml #\"Python library which allows for easy handling of XML and HTML files, and can also be used for web scraping.\"\n",
    "\n",
    "#IMPORTS\n",
    "\n",
    "import requests #\"allows you to send HTTP requests using Python. The HTTP request returns a Response Object with all the response data (content, encoding, status, etc).\"\n",
    "import bs4 #\"Beautiful Soup is a Python library for pulling data out of HTML and XML files.\"\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia #\"Wikipedia is a Python library that makes it easy to access and parse data from Wikipedia. Search Wikipedia, get article summaries, get data like links and images from a page, and more. Wikipedia wraps the MediaWiki API so you can focus on using Wikipedia data, not getting it.\"\n",
    "import pandas as pd #\"pandas is a popular Python-based data analysis toolkit which can be imported using import pandas as pd . It presents a diverse range of utilities, ranging from parsing multiple file formats to converting an entire data table into a NumPy matrix array.\"\n",
    "from urllib.request import urlopen #After doing a little bit of research i found this could be pretty useful for web scraping, it allows us to actually open URL's.\n",
    "\n",
    "#FUNCTIONS\n",
    "\n",
    "szn =  2003 #int(input(\"Enter a Year: \")) #This is the season we will be analyzing. This is pretty cool because I can switch this to any year and it will give me the stats for that season.\n",
    "url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\".format(szn) #This link is where we will be web scraping from. I decided to web scrape rather than use a CVS file, because NBA statistic are dynamic and alawys changing.\n",
    "html = urlopen(url) # this is the HTML from the given URL\n",
    "soup = BeautifulSoup(html) # this function is what scrapes through our URL, to turn it into something we can actually use.\n",
    "soup.findAll('tr', limit=2) #the findall function is what allows us to get the column headers. We need these to better organize and understand the data.\n",
    "headers = [th.getText() for th in soup.findAll('tr', limit=2)[0].findAll('th')] # the getText() function allows us to take the text we need from the URL and put it into a list\n",
    "headers = headers[1:] # 1: gets rid of the first column. The \"ranking order\" is unnecessary and irelevant for us.\n",
    "headers\n",
    "rows = soup.findAll('tr')[1:] # we again are skipping past the \"rankings\" column and the firt row\n",
    "player_stats = [[td.getText() for td in rows[i].findAll('td')]\n",
    "            for i in range(len(rows))]\n",
    "df = pd.DataFrame(player_stats, columns = headers) #THIS IS WHAT ALLOWS US TO CREATE THE DATA FRAME\n",
    "df.head(10) #SHOWS US THE TOP 10 ROWS IN THE DATA FRAME\n",
    "\n",
    "ray_allen_frame = df[df['Player']== 'Ray Allen*']\n",
    "ray_allen_frame\n",
    "\n",
    "ray_allen_frame_total = ray_allen_frame[ray_allen_frame['Tm'] == 'TOT']\n",
    "ray_allen_frame_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
